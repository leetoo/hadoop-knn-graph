%\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

%\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

% --------------------- Tibo
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{array}

\usepackage{hyperref}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
%\usetikzlibrary{plotmarks}

\usepackage{relsize}
\usepackage{amssymb}
\usepackage{flushend}

\usepackage{xcolor}
\usepackage{changebar}

\newcommand{\removed}[1]{\cbstart\removedfragile{#1}\cbend{}}
\newcommand{\removedfragile}[1]{{\color{red}{#1}}{}}
\newcommand{\added}[1]{\cbstart\addedfragile{#1}\cbend{}}
\newcommand{\addedfragile}[1]{{\color{green!50!black}{#1}}{}}
\newcommand{\changed}[2]{\added{#1}\removed{#2}}

\setlength\extrarowheight{3pt}
%\setlength{\belowcaptionskip}{15pt}
% --------------------- Tibo


% The booktabs package is used by this sample document
% (it provides \toprule, \midrule and \bottomrule).
% Remove the next line if you don't require it.
%\usepackage{booktabs}

% The siunitx package is used by this sample document
% to align numbers in a column by their decimal point.
% Remove the next line if you don't require it.
%\usepackage[load-configurations=version-1]{siunitx} % newer version
%\usepackage{siunitx}

% The following command is just for this sample document:
%\newcommand{\cs}[1]{\texttt{\char`\\#1}}

\jmlrvolume{29}
\jmlryear{2014}
\jmlrworkshop{BIGMINE 2014}

\title[Scalable Graph Building from Text Data]{Scalable Graph Building from Text Data}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \author{\Name{Author Name1} \Email{abc@sample.com}\and
 %  \Name{Author Name2} \Email{xyz@sample.com}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \author{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \Name{Author Name4} \Email{an4@sample.com}\\
 %  \Name{Author Name5} \Email{an5@sample.com}\\
 %  \Name{Author Name6} \Email{an6@sample.com}\\
 %  \Name{Author Name7} \Email{an7@sample.com}\\
 %  \Name{Author Name8} \Email{an8@sample.com}\\
 %  \Name{Author Name9} \Email{an9@sample.com}\\
 %  \Name{Author Name10} \Email{an10@sample.com}\\
 %  \Name{Author Name11} \Email{an11@sample.com}\\
 %  \Name{Author Name12} \Email{an12@sample.com}\\
 %  \Name{Author Name13} \Email{an13@sample.com}\\
 %  \Name{Author Name14} \Email{an14@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
  \author{\Name{Thibault Debatty} \Email{thibault.debatty@rma.ac.be}\\
  \addr Royal Military Academy, Brussels, Belgium
  \AND
  \Name{Pietro Michiardi} \Email{pietro.michiardi@eurecom.fr}\\
  \addr EURECOM, Campus SophiaTech, France
  \AND
  \Name{Olivier Thonnard} \Email{olivier\_thonnard@symantec.com}\\
  \addr Symantec Research Labs, Sophia Antipolis, France
  \AND
  \Name{Wim Mees} \Email{wim.mees@rma.ac.be}\\
  \addr Royal Military Academy, Brussels, Belgium
 }

%\editors{List of editors' names}

\begin{document}

\maketitle

\begin{abstract}
In this paper we propose NNCTPH, a MapReduce algorithm that builds an approximate $k$-NN graph from large text datasets. The algorithm uses a modified version of Context Triggered Piecewise Hashing to bin the input data into buckets, and uses brute-force inside the buckets to build the graph. It also uses multiple stages to join the different unconnected subgraphs. We experimentally test the algorithm on different datasets consisting of the subject of spam emails.  Although the algorithm is still at an early development stage, it already proves to be four times faster than a MapReduce implementation of NNDescent, for the same quality of produced graph.
\end{abstract}

\begin{keywords}
$k$-NN graph, graph building, MapReduce, text data, Context-Triggered Piecewise Hashing
\end{keywords}

\section{Introduction}
A graph is a mathematical structure used to represent relations between objects. A graph is made up of nodes (or vertices) connected with edges. In some cases, the edges have a weight, resulting in a weighted graph. Graph theory is a very ancient topic, dating back from 1736 \cite{Biggs1986}. It has received a strong highlight these last years with the explosion of web search engines and social networks like Facebook and Twitter. Indeed, the data generated by these networks and the web itself can easily be formatted as graphs, making graph algorithms a premium choice for processing the data.

As a consequence, a lot of research has been devoted to efficiently analyze this kind of data, either sequentially or in parallel, like in \cite{Rajaraman2010b}, \cite{Liben2005}, \cite{Broder:2000:GSW:347319.346290} or \cite{Scott:2011:SHS:2392673}.

In a lot of cases however, we may wish to analyze unstructured data, that is not formatted as a graph. Nevertheless, it can be interesting to convert the dataset into a graph, to be able to use these algorithms. In most cases, the trick used is to build a k-nearest neighbors graph ($k$-NN graph), where each node is connected to (has an edge to) its $k$ nearest neighbors, according to a given similarity metric.

Another possibility would be to create an $\epsilon$-NN graph, a graph where an edge exists between two nodes if their distance is less than a pre-defined threshold $\epsilon$. However, it has been shown in \cite{Belkin:2003:LED:795523.795528} that $\epsilon$-NN graphs easily result in disconnected components. Moreover, it is usually difficult to find a good value of $\epsilon$ which yields graphs with an appropriate number of edges \cite{Chen2009}. From a practical point of view, it is more efficient to build a $k$-NN graph and afterward filter the graph with different values of $\epsilon$. Hence most of the research on graph building currently focus on $k$-NN graphs.

\section{Related work}

\subsection{Nearest neighbor search}
$k$-NN graph building algorithms are of course closely related to nearest neighbor search algorithms, and also to the k-nearest neighbors method.

The nearest-neighbor search problem is formally defined as follows: given a set $S$ of points in a space $M$ and a so-called query point $q \in M$, find the closest point in $S$ to $q$. The $k$-NN search is a direct generalization of this problem, where we need to find the $k$ closest points.

But $k$-NN search is also a method used for classification and regression. In $k$-NN classification, the algorithm assigns to an object the class which is most common amongst its $k$ nearest neighbors in the training set. In the case of regression, the value computed by the algorithm is the average value of the $k$ nearest neighbors of the object in the training set.

Different methods exist to find the nearest neighbor, or the k-nearest neighbors, of a point. The naive method, also called linear search, consists in computing the distance between the query point and every other point in the set, keeping track of the "best so far" (or $k$ "best so far").

Some techniques rely on the branch and bond algorithm, with some kind of index to partition the space $M$. For example, a k-d tree, that recursively partition the space into equally sized sub-spaces can be used to speedup search, like in \cite{Moore1991}. R-trees can also be used for euclidean spaces. In the case of generic metric spaces, vantage-point trees and BK-trees can be used.

Finally, some algorithms use Locality-Sensitive Hashing (LSH), like \cite{Rajaraman2010}. LSH is originally a method used to perform probabilistic dimension reduction of high-dimensional data. The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability. At the opposite of conventional hash functions, such as those used in cryptography, in the case of LSH the goal is to maximize the probability of "collision" between similar items.

\subsection{$k$-NN graph building algorithms}

The most naive way to build a $k$-NN graph is of course using brute force to compute all pairwise similarities. This method has a computational cost of $O(n^2)$.

A more subtle way consists in iteratively using a nearest neighbor search algorithm, like the ones presented above, to find the neighbors of all nodes in the dataset.
404917
Lately, different algorithms have been proposed specifically to efficiently build a $k$-NN graph from a dataset. Most of them will naturally share similarities with nearest neighbor search algorithms.

In \cite{Paredes2006}, the authors propose two algorithms that first build an index of the dataset to reduce the number of distances that have to be computed:

\begin{itemize}
	\item A recursive partition based algorithm: In the first stage, the algorithm builds the index by performing a recursive partition of the space. In the second stage, it builds the $k$-NN graph by searching the $k$-NN of each node, using the order induced by the partitioning.
	\item A pivot based algorithm: The algorithm first build a pivot index. Then, the $k$-NN graph is built by performing range-optimal queries improved with metric and graph considerations.
\end{itemize}

Both algorithms are not limited to euclidean space and support metric spaces. The authors tested the algorithms using text data and edit distance as measure of similarity.

In \cite{Connor2009}, the authors present a distributed algorithm, but that requires a shared memory architecture to store a shared kd-tree based index.

In a lot of cases, to achieve a higher speedup, the designed algorithms focus on building an approximate $k$-NN graph.

A versatile algorithm to efficiently compute an approximate $k$-NN graph is described in \cite{Dong2011}. The algorithm, called NN-Descent, starts by creating edges between random nodes. Then, for each node, it computes the similarity between all neighbors of the current neighbors, to find better edges. The algorithm iterates until it cannot find better edges anymore. The main advantage of this algorithm is that it works with any similarity measure. Dong et al. experimentally found the computational cost is around $O(n^{1.14})$.

They also propose a MapReduce version of the algorithm. Internally, the algorithm works with a kind of adjacency list called neighbors list. It holds the candidate neighbors of a node. The algorithm first creates a random neighbors list for each node. Then each iteration is realized with two MapReduce jobs. First, the mapper emits the initial neighbors list, and reverses the neighbors list to produce and emit new candidate neighbors. The reducer merges all neighbors for each node, to produce a new extended neighbors list. In the second job, the mappers compute and emit the pairwise similarity between all elements of each neighbors list. Finally, the reducer merges the neighbors of each key node, keeping only the $k$ neighbors with the highest similarity.

A sequential C++ implementation of the algorithm is also available under the name KGraph \cite{kgraph}.

Various authors propose algorithms relying on locality-sensitive hashing.

In \cite{Hsieh2012}, the authors propose a MapReduce algorithm that first bins the scale-invariant feature transform (SIFT) description of images into overlapping pools. The algorithm then computes the pairwise similarity between images in the same bucket to build the $k$-NN graph of images. For binning, the algorithm uses MinHash, a variant of LSH that uses Jaccard coefficient as similarity measure.

Similarly, in \cite{Zhang2013}, the authors use LSH to divide the dataset into small groups. Then, inside these small groups, they use the algorithm proposed by Dong et al. to build the $k$-NN graph. As groups are not overlapping, the constructed graph is a union of multiple isolated small graphs. To bind the final graph, and improve the approximation quality, the division is repeated several times to generate multiple approximate graphs, which are finally combined to produce the final graph.

Furthermore, the authors propose a method to produce equally sized groups, thus alleviating the computational cost of skewed data. They first project the item's hash code on a random direction. Then they sort items by their projection values. Finally, they divide this sequence of items into equally sized buckets. By doing so, the items with same hash code will fall in the same bucket with a high probability.

Finally, Zhang et al. show experimentally that their algorithm is much faster than existing algorithms, for similar quality of the built graph.

When it comes to building a $k$-NN graph from a big unstructured text dataset, none of the previous algorithms offers a efficient solution. Algorithms that rely on indexes are hard to implement in parallel on a distributed memory architecture like MapReduce. As LSH functions are defined only for some similarity measures ($l_p$, Mahalanobis distance, kernel similarity, and $\chi^2$ distance), the algorithms relying on LSH cannot be used when it comes to build a $k$-NN graph from text data using edit distance (Levenshtein distance) or any similar distance metric (weighted Levenshtein distance, Jaro-Winkler distance, Hamming distance) as a similarity measure.

In the case of NNDescent, the MapReduce version of the algorithm requires two MR jobs per iteration, and multiple iterations to converge. Moreover, the algorithm requires to read and write a lot of data on disk between jobs. Although the sequential version of the algorithm proved to be very efficient, these constraints make it very inefficient when implemented in parallel. This will be confirmed during the experimental tests presented below.

\section{Building a graph from a big text dataset}

As no current algorithm is suited for building a $k$-NN graph from a big text dataset, we propose here a new algorithm. The algorithm requires a single iteration and a single MapReduce job, and it does not rely on a shared index. Internally, it uses a specific hashing scheme, called Context Triggered Piecewise Hashing (CTPH). Hence we call the algorithm NNCTPH.

\subsection{Context Triggered Piecewise Hashing}

Context Triggered Piecewise Hashing (CTPH), also called Fuzzy Hashing, is a hashing function that tends to produce the same hash for similar input strings. It was originally developed by Tridgell as a spam email detector called SpamSum \cite{SpamSum}. The algorithm is used to build a database of hashes of known spams. When a new email is received, its hash is computed, and compared with the spam database. If a similar hash is found, the incoming email is considered as spam, and discarded.

The algorithm works by splitting a character string in chunks of variable length. The end point of a chunk is determined by a rolling hash. This rolling hash is based on the Adler-32 checksum used in the zlib compression library \cite{zlib}. It uses a window of 7 characters that slides over the input string. By using a rolling hash, the algorithm can perform auto resynchronisation if characters are inserted or deleted between two strings. If the value of the rolling hash matches a given value, the end of current chunk is found.

As the final hash of the input string is a sequence of characters that correspond to base64 encoding, each chunk is hashed into a single character out of 64 possibilities. The block hash function used therefor is based on the Fowler/Noll/Vo (FNV) hash function \cite{FNV}. In the original algorithm, the matching value for the rolling hashing is chosen in such a way that the final hash has a length of 64 characters.

Fuzzy hashing is also known under the name ssdeep \cite{ssdeep}, which is the name of the tool implemented by Kornblum. In \cite{Kornblum200691}, he uses ssdeep to identify almost identical files to support computer forensics.

\subsection{NNCTPH}

We propose here to use CTPH to build a $k$-NN graph from text data using MapReduce. The algorithm, called NNCTPH, is presented in Algorithms~\ref{algo-nnctph-map} and~\ref{algo-nnctph-reduce}. It requires a single MapReduce job. In the map phase, the algorithm uses CTPH to produce a hash of each input string. This hash value is then used to bin the string into a bucket. Each reduce task uses brute-force to compute pairwise similarities between all strings of a single bucket, and for each node emits the $k$ edges with the highest similarity.

\begin{algorithm}
\caption{NNCTPH Map}
\label{algo-nnctph-map}
\begin{algorithmic}
%bin-map(string) // PM: Can you describe "string"
%  node = Parse(string) // PM: Can you also (separately) descibe Parse?
%  hash = Hash(node.value, levels) // PM: You return an array of hashes of length levels?
%  for (level in 0 .. stages-1) :
%    hash_key = level - hash[stage]
%    Emit(hash_key => node) // PM: Could you use the <k, v> notation?
\State Input: $stages, hash\_length, hash\_letters$
\State
\Procedure{Map}{$\mathit{string}$}
  \State $hash$ = CTPH($string$, $stages$, $hash\_length$, $hash\_letters$)
  \For{$s$ in $0 .. stages$}
    \State Emit($s\_hash[s] \Rightarrow string$)
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{NNCTPH Reduce}
\label{algo-nnctph-reduce}
\begin{algorithmic}
\State Input: $k, stages$
\State
\Procedure{Reduce}{$\mathit{key}, <\mathit{strings}>$}
  \State $k' = k / stages$
  \State ReadAllStrings()
  \State ComputePairwiseSimilarities()
  \For{$s$ in $strings$}
    \State edges = FindKNN($s, k'$)
    \State Emit($edges$)
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

To control the number of buckets, and hence the average number of nodes per bucket, we modified the original CTPH function to:

\begin{itemize}
	\item produce a hash of variable size;
	\item use only a subset of letters in the hash, instead of the 64 original letters.
\end{itemize}

For example, by using a hash of two characters with ten possible letters ("A" to "J"), we create 100 buckets. The number of buckets will have an influence on the average number of nodes per bucket, but also on the parallelism of the algorithm, and hence on the processing time and on the quality of the final graph.

Doing this, we would end up with a series of unconnected subgraphs, as no edges are created between the nodes of different buckets. To avoid this and reconnect the graph, in the map phase we create a longer hash (using a coefficient we call \texttt{stages}) and emit the input string once for each subpart of the hash. Then, each reducer emits $stages/k$ edges for each string. For more clarity, the hash returned by our CTPH funcion is an array of $stages$ elements.

For example, to create 100 buckets and two stages, our CTPH function produces a hash of four characters, using ten letters. The returned value is an array of two strings, each consisting of two characters. If the hash of an input string is "ABCD", the returned value is an array "AB", "CD", and the original string will be emitted twice by the mapper: once for "AB", and once for "CD". In this way, we can expect that the reduce task for bucket "CD" will produce edges to nodes located outside bucket "AB", hence reconnecting the graph.

The number of stages used will also have an impact on the quality of the graph, and on the quantity of data to shuffle and transmit over the network.

Our algorithm thus requires three parameters: the number of stages, the number of characters in a hash, and the number of letters used to produce the hash. These have an impact on the quality of the graph, on the quantity of data that has to be shuffled, on the parallelism of the algorithm, on the number of similarities to compute, and the quantity of RAM required by the reducers. In the future, we plan to perform an in-depth study of the effects and interactions of these parameters.

\section{Experimental evaluation}

To experimentally test our algorithm, we implement it using Hadoop and test it on four datasets containing the subject of spam emails. We also compare it against a Hadoop implementation of NNDescent, and a Hadoop implementation of the brute-force method. All algorithms are executed on a cluster of four servers, each equipped with two quad-core processors and 16GB of RAM.

To compute the similarity between spam subjects, we use Jaro-Winkler \cite{winkler90}. This measure of string similarity is normalized such that 0 equates to no similarity and 1 is an exact match.

\subsection{Graph quality}

To compare the accuracy of algorithms, like in \cite{Dong2011}, we use recall to measure the quality of produced graphs. The ground truth is the true $k$-NN graph obtained using the naive, brute-force, algorithm. The recall of a single node is the number of its correct edges divided by $k$. The recall of an approximate $k$-NN graph is the average recall of all nodes.

For the tests, we use a dataset containing the subject of 200.000 spam emails, and want to build a $10$-NN graph. For NNCTPH, we use two stages, hashes of two characters, and we let the number of possible characters vary between 31 and 40. Doing so, we let the number of buckets vary between 961 and 1600, and the average number of spams per bucket vary between 208 and 125. The resulting execution time and recall are displayed in Table~\ref{table:nnctph:characters}.

\begin{table}[ht]
  \caption{Running time and recall of NNCTPH algorithm on a dataset of 200.000 spams\newline with two stages and hashes of two characters, to build a $10$-NN graph}
  \label{table:nnctph:characters} 
  \centering
  \begin{tabular}{p{3.7cm} *{5}{p{1.8cm}}}
    \hline
    					& \textbf{T1}   & \textbf{T2}	& \textbf{T3} 	& \textbf{T4}	& \textbf{T5}  	\\
    \hline
    Characters				& 31		& 32		& 33		& 34		& 35		\\
    Buckets				& 961		& 1024		& 1089		& 1156		& 1225		\\
    Running time (sec)  		& 153		& 207		& 136		& 142        	& 118		\\
    Similarities\newline(x 1000.000)	& 160		& 164		& 156		& 146		& 143		\\
    Recall				& 23.39\%	& 23.40\%	& 23.17\%	& 23.24\%	& 23.25\%	\\
    \hline

    					& \textbf{T6}   & \textbf{T7}	& \textbf{T8} 	& \textbf{T9}	& \textbf{T10} 	\\
    \hline
    Characters				& 36		& 37		& 38		& 39		& 40		\\
    Buckets				& 1296		& 1369		& 1444		& 1521		& 1600		\\
    Running time (sec)  		& 129		& 141		& 115		& 157         	& 174		\\
    Similarities\newline(x 1000.000)	& 138		& 138		& 133		& 129		& 133		\\
    Recall				& 23.18\%	& 23.05\%	& 23.04\%	& 23.08\%	& 23.08\%	\\
    \hline

  \end{tabular} 
\end{table}

As we can notice, the number of edges that are correctly found is fairly stable, around 23\%. This might seem low, but given the size of our dataset, rapidly discovering this amount of correct edges is not trivial. The average running time of our algorithm for these tests is 140 sec. As a matter of comparison, the brute-force algorithm we use to compute ground truth takes approximatively 9h to complete on the same hardware.

We also compare our algorithm with a MapReduce implementation of NNDescent. Table~\ref{table:nndescent} shows the running time and recall of 10 iterations of NNDescent on the same dataset. As we can see, the algorithm requires 8 or 9 iterations to achieve the same result. For the same quality of the resulting graph, our algorithm is thus 4 times faster than NNDescent.

\begin{table}[ht]
  \caption{Running time and recall of NNDescent algorithm on a dataset of 200.000 spams, to build a $10$-NN graph}
  \label{table:nndescent} 
  \centering
  \begin{tabular}{p{3.7cm} *{5}{p{1.8cm}}}
    \hline
    \textbf{Iterations}			& \textbf{1}   	& \textbf{2}	& \textbf{3} 	& \textbf{4}	& \textbf{5}  	\\
    \hline
    Running time (sec)  		& 134		& 203           & 272		& 339         	& 406		\\
    \# similarities\newline(x 1000.000)	& 22		& 44		& 66		& 88		& 110		\\
    Recall				& 0.053\%	& 0.37\%	& 1.60\%	& 5.57\%	& 12.37\%	\\
    \hline

    \textbf{Iterations}			& \textbf{6}   	& \textbf{7}	& \textbf{8} 	& \textbf{9}	& \textbf{10}  	\\
    \hline
    Running time (sec)  		& 472 		& 537           & 604		& 670          	& 736		\\
    Similarities\newline(x 1000.000)	& 132		& 154		& 176		& 198		& 220		\\
    Recall				& 18.20\%	& 21.57\%	& 23.09\%	& 23.77\%	& 24.07\%	\\
    \hline
  \end{tabular} 
\end{table}

Moreover, even when we execute more iterations of NNDescent, the algorithm has difficulties to reach a recall higher than 24\%. This is shown on Figure~\ref{figure:nndescent-30}. It shows the evolution of recall for up to 30 iterations of NNDescent. As we can see, starting from iteration 9, the recall only rises very slowly. This is probably due to the structure of our dataset, where some additional edges are difficult to find because they are "hidden" amongst a lot of other edges of similar but slightly inferior similarity.

\begin{figure}[ht]
  \centering

  \begin{tikzpicture}

  \begin{axis}[
  ylabel={Recall (\%)},
  xlabel={Iterations},
  width={10cm},
  height={6cm},
  scale only axis]

  \addplot [
    color=blue,
    only marks,
    mark=*,
    mark options={solid}
  ] table{nndescent-30-recall.dat};

  \end{axis}
\end{tikzpicture}

\caption{Evolution of recall for NNDescent algorithm with up to 30 iterations\newline for a dataset of 200.000 spam subjects}
\label{figure:nndescent-30}
\end{figure}

On Table~\ref{table:nnctph:characters} we can observe that, as we could expect, when the number of letters and thus the number of buckets rise, the number of computed similarities roughly decreases. But it is not necessarily the case of the computing time. This is mainly due to the fact that the data sent by the mappers to the reduce tasks is skewed. The computing time is thus dominated by these few reduce tasks that have a lot more similarities to compute than the others.

This is confirmed on Figure~\ref{figure:frequency} that shows the frequency distribution of the number of nodes per bucket (and thus per reduce task). NNCTPH is used on the dataset of 200.000 spams, with two stages, hashes of two characters, and 32 letters. We thus have 1024 buckets, and an average of 195 strings per bucket. As we can see, there is a peak around 100 nodes per bucket, but there are also a lot of buckets with more than 1000 nodes, and even one with approximatively 7000 nodes. As the algorithm uses brute-force to compute edges inside buckets, these one will have much more similarities to compute, and will mostly be responsible for the execution time.

\begin{figure}
  \centering
  \newlength\figureheight
  \newlength\figurewidth
  \setlength\figureheight{6cm}
  \setlength\figurewidth{10cm}

\begin{tikzpicture}

  \begin{axis}[%
  xlabel={Number of nodes per bucket},
  ylabel={Number of buckets},
  xmode = log,
  width=\figurewidth,
  height=\figureheight,
  scale only axis]

  \addplot [
    color=blue,
    only marks,
    mark=*,
    mark options={solid}
  ] table{freqs.dat};

  \end{axis}
\end{tikzpicture}

\caption{Frequency distribution of the number of nodes per bucket\newline when using NNCTPH to create a graph from a dataset of 200.000 spams\newline with two stages, hashes of two characters, and 32 letters (1024 buckets)}
\label{figure:frequency}
\end{figure}

To alleviate this problem, we plan to use NNDescent instead of brute-force to find nearest-neighbors inside buckets. This algorithm has proved to be very efficient when implemented sequentially, and does not require $O(n^2)$ similarity computations. Handling skewed data is not a trivial problem, and has already been studied in the context of distributed databases for example \cite{Xu:2008:HDS:1376616.1376720}. To improve the performance of NNCTPH we will have to tackle this problem. Amongst other possibilities we plan to apply extendible hashing.

\subsection{Scalability}

To test the scalability of our algorithm, we run it against different datasets containing 200.000 to 800.000 spam emails. We use two stages, hashes of two characters, and 64 letters (4096 buckets). This time we don't have the ground-truth to compute recall, but we compute the average similarity of computed edges. We compare the obtained results with NNDescent algorithm running a fixed number of eight iterations.

To compare the performance and computational cost of the algorithms, as the number of similarities to compute depends on the size of the dataset, we use the scan rate, defined as follows:

$$ \text{scan rate} = \frac{\text{\# similarity evaluations}}{n (n-1) / 2} $$

where $n (n-1) / 2$ is the number of similarities computed by the naive algorithm.

\begin{table}[h]
  \caption{Running time, scan rate and average similarity of edges found by NNCTPH and NNDescent on different datasets}
  \label{table:nnctph} 
  \centering
  \begin{tabular}{p{3.4cm} *{4}{p{2.4cm}}}
    \hline
    \textbf{Dataset}   			& \textbf{spam200k}   	& \textbf{spam400k}   	& \textbf{spam600k} 	& \textbf{spam800k}  	\\
    Spams           			& 200.000             	& 400.000             	& 600.000		& 800.000            	\\
    \hline
    \textbf{NNCTPH} \\
    Running time (sec)  		& 97	           	& 316           	& 675			& 1093          	\\
    Similarities\newline(x 1000.000)	& 88			& 359			& 773			& 1346			\\
    Scan rate				& 0.44\%		& 0.45\%		& 0.43\%		& 0.42\%		\\
    Average similarity			&			&			&			&			\\
    \hline
    \textbf{NNDescent} \\
    Running time (sec)  		& 604	           	& 842			& 			& 			\\
    Similarities\newline(x 1000.000)	& 132			& 352			& 			& 			\\
    Scan rate				& 6.66\%		& \%		& \%		& \%		\\
    Average similarity			& 			&			&			&			\\
    \hline

  \end{tabular} 
\end{table}




\section{Conclusions and future work}

In this paper we propose NNCTPH, a MapReduce algorithm that builds an approximate $k$-NN graph from large text datasets. The algorithm uses a modified version of Context Triggered Piecewise Hashing to bin the input data into buckets, and uses brute-force inside the buckets to build the graph. It also uses multiple stages to join the different unconnected subgraphs.

We experimentally test the algorithm on different datasets consisting of the subject of spam emails.  Although the algorithm is still at an early development stage, it already proves to be four times faster than a MapReduce implementation of NNDescent, for the same quality of produced graph.

In the future we plan to:
\begin{itemize}
  \item Use NNDescent inside the buckets, as this algorithm has to be very efficient when implemented sequentially;
  \item Study the influence of the different parameters of the algorihm on the quality of the resulting graph, and on the performance of the algorithm (computational cost, parallelization, network traffic);
  \item Study the problem of skewed data;
  \item Study the influence of graph quality on the algorithms applied afterward on the graph (connected components for example);
  \item Test the scalability of the algorithm on bigger datasets and bigger clusters;
  \item Compare the algorithm with algorithms that build a $k$-NN graph using the bag-of-words (BOW) model.
\end{itemize}

\acks{This work has been partially supported by the EU project BigFoot (FP7-ICT-317858).}

\bibliography{bigmine2014_debatty}


\end{document}
