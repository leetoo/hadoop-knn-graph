%\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

%\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

% --------------------- Tibo
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{array}

\usepackage{hyperref}

\usepackage{pgfplots}
\usepackage{pgfplotstable}
%\usetikzlibrary{plotmarks}

\usepackage{relsize}
\usepackage{amssymb}
\usepackage{flushend}

\usepackage{xcolor}
\usepackage{changebar}

\newcommand{\removed}[1]{\cbstart\removedfragile{#1}\cbend{}}
\newcommand{\removedfragile}[1]{{\color{red}{#1}}{}}
\newcommand{\added}[1]{\cbstart\addedfragile{#1}\cbend{}}
\newcommand{\addedfragile}[1]{{\color{green!50!black}{#1}}{}}
\newcommand{\changed}[2]{\added{#1}\removed{#2}}
% --------------------- Tibo


% The booktabs package is used by this sample document
% (it provides \toprule, \midrule and \bottomrule).
% Remove the next line if you don't require it.
%\usepackage{booktabs}

% The siunitx package is used by this sample document
% to align numbers in a column by their decimal point.
% Remove the next line if you don't require it.
%\usepackage[load-configurations=version-1]{siunitx} % newer version
%\usepackage{siunitx}

% The following command is just for this sample document:
%\newcommand{\cs}[1]{\texttt{\char`\\#1}}

\jmlrvolume{29}
\jmlryear{2014}
\jmlrworkshop{BIGMINE 2014}

\title[Short Title]{Efficient Graph Building from Text Data}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \author{\Name{Author Name1} \Email{abc@sample.com}\and
 %  \Name{Author Name2} \Email{xyz@sample.com}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \author{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \Name{Author Name4} \Email{an4@sample.com}\\
 %  \Name{Author Name5} \Email{an5@sample.com}\\
 %  \Name{Author Name6} \Email{an6@sample.com}\\
 %  \Name{Author Name7} \Email{an7@sample.com}\\
 %  \Name{Author Name8} \Email{an8@sample.com}\\
 %  \Name{Author Name9} \Email{an9@sample.com}\\
 %  \Name{Author Name10} \Email{an10@sample.com}\\
 %  \Name{Author Name11} \Email{an11@sample.com}\\
 %  \Name{Author Name12} \Email{an12@sample.com}\\
 %  \Name{Author Name13} \Email{an13@sample.com}\\
 %  \Name{Author Name14} \Email{an14@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
  \author{\Name{Thibault Debatty} \Email{thibault.debatty@rma.ac.be}\\
  \addr Royal Military Academy, Brussels, Belgium
  \AND
  \Name{Pietro Michiardi} \Email{pietro.michiardi@eurecom.fr}\\
  \addr EURECOM, Campus SophiaTech, France
  \AND
  \Name{Olivier Thonnard} \Email{olivier\_thonnard@symantec.com}\\
  \addr Symantec Research Labs, Sophia Antipolis, France
  \AND
  \Name{Wim Mees} \Email{wim.mees@rma.ac.be}\\
  \addr Royal Military Academy, Brussels, Belgium
 }

%\editors{List of editors' names}

\begin{document}

\maketitle

\begin{abstract}
This is the abstract for this article.
\end{abstract}

\begin{keywords}
List of keywords
\end{keywords}

\section{Introduction}
A graph is a mathematical structure used to represent relations between objects. A graph is made up of nodes (or vertices) connected with edges. In some cases, the edges have a weight, resulting in a weighted graph. Graph theory is a very ancient topic, dating back from 1736. %http://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg
It has received a strong highlight these last years with the explosion of social networks like Facebook and Twitter. Indeed, the data generated by these networks can easily be formatted as a graph, making graph algorithms a premium choice for processing the data.

As a consequence, a lot of algorithms have been developed, and implemented, to efficiently analyze this kind of data, either sequentially or in parallel. XXXXXXX add examples of graph algorithms connected components, clustering, ...

In a lot of cases, we may have to analyze unstructured data, that is not formatted as a graph. Nevertheless, it can be interesting to convert the dataset into a graph, to be able to use these algorithms. In most cases (XXXXXXXXX add reference), the trick used is to build a k-nearest neighbors graph (k-NN graph), a graph where each node is connected to (has an edge to) its k nearest neighbors, according to a given similarity metric.

\section{Related work}

\subsection{Nearest neighbor search}
k-NN graph building algorithms are of course closely related to nearest neighbor search algorithms, and also to the k-nearest neighbors method.

The nearest-neighbor search problem is formally defined as follows: given a set $S$ of points in a space $M$ and a so-called query point $q \in M$, find the closest point in $S$ to $q$. The k-NN search is a direct generalization of this problem, where we need to find the k closest points.

But k-NN search is also a method used for classification and regression. In k-NN classification, the algorithm assigns to an object the class which is most common amongst its k nearest neighbors in the training set. In the case of regression, the value computed by the algorithm is the average value of the k nearest neighbors of the object in the training set.

Different methods exist to find the nearest neighbor, or the k-nearest neighbors, of a point. The naive method, also called linear search, consists in computing the distance between the query point and every other point in the set, keeping track of the "best so far" (or k "best so far").

Some techniques rely on the branch and bond algorithm, with some kind of index to partition the space $M$. For example, a k-d tree, that recursively partition the space into equally sized sub-spaces can be used to speedup search, like in \cite{Moore1991}. R-trees can also be used for euclidean spaces. In the case of generic metric spaces, vantage-point trees and BK-trees can be used.

XXXXXX add explanation and references!

Finally, some algorithms use Locality-Sensitive Hashing (LSH), like \cite{Rajaraman2010}. LSH is originally a method used to perform probabilistic dimension reduction of high-dimensional data. The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability. At the opposite of conventional hash functions, such as those used in cryptography, in the case of LSH the goal is to maximize the probability of "collision" between similar items.

\subsection{k-NN graph building algorithms}

The most naive way to build a k-NN graph is of course using brute force to compute all pairwise similarities. This method has a computational cost of $O(n^2)$.

A more subtle way consists in iteratively using a nearest neighbor search algorithm, like the ones presented above, to find the neighbors of all nodes in the dataset.

Lately, different algorithms have been proposed specifically to efficiently build a k-NN graph from a dataset. Most of them will naturally share similarities with nearest neighbor search algorithms.

In \cite{Paredes2006}, the authors propose two algorithms that first build an index of the dataset to reduce the number of distances that have to be computed:

\begin{itemize}
	\item A recursive partition based algorithm: In the first stage, the algorithm builds the index by performing a recursive partition of the space. In the second stage, it builds the k-nn graph by searching the k-nn of each node, using the order induced by the partitioning.
	\item A pivot based algorithm: The algorithm first build a pivot index. Then, the k-nn graph is built by performing range-optimal queries improved with metric and graph considerations.
\end{itemize}

Both algorithms are not limited to euclidean space and support metric spaces. The authors tested the algorithms using text data and edit distance as measure of similarity.

In \cite{Connor2009}, the authors present a distributed algorithm, but that requires a shared memory architecture to store a shared kd-tree based index.

In a lot of cases, to achieve a higher speedup, the designed algorithms focus on building an approximate k-nn graph.

A versatile algorithm to efficiently compute an approximate k-NN graph is described in \cite{Dong2011}. The algorithm, called NN-Descent, starts by creating edges between random nodes. Then, for each node, it computes the similarity between all neighbors of the current neighbors, to find better edges. The algorithm iterates until it cannot find better edges anymore. The main advantage of this algorithm is that it works with any similarity measure. Dong et al. experimentally found the computational cost is around $O(n^{1.14})$.

They also propose a MapReduce version of the algorithm. Internally, instead of working with edges, each node has a neighbors list, a list of candidate neighbors. The algorithm first creates a random neighbors list for each node. Then each iteration is realized with two MapReduce jobs. First, the mapper emits the initial neighbors list, and reverses the neighbors list to produce new candidate neighbors. The reducer merges all neighbors for each node, to produce a new extended neighbors list. In the second job, the mappers compute and emit the pairwise similarity between all elements of each neighbors list. Finally, the reducer merges the neighbors of each key node, keeping only the $k$ neighbors with the highest similarity.

Various authors propose algorithms relying on locality-sensitive hashing.

In \cite{Hsieh2012}, the authors propose a MapReduce algorithm that first bins the scale-invariant feature transform (SIFT) description of images into overlapping pools. The algorithm then computes the pairwise similarity between images in the same bucket to build the k-nn graph of images. For binning, the algorithm uses MinHash, a variant of LSH that uses Jaccard coefficient as similarity measure.

Similarly, in \cite{Zhang2013}, the authors use LSH to divide the dataset into small groups. Then, inside these small groups, they use the algorithm proposed by Dong et al. to build the k-nn graph. As groups are not overlapping, the constructed graph is a union of multiple isolated small graphs. To bind the final graph, and improve the approximation quality, the division is repeated several times to generate multiple approximate graphs, which are finally combined to produce the final graph.

Furthermore, the authors propose a method to produce equally sized groups, thus alleviating the computational cost of skewed data. They first project the item's hash code on a random direction. Then they sort items by their projection values. Finally, they divide this sequence of items into equally sized buckets. By doing so, the items with same hash code will fall in the same bucket with a high probability.

Finally, Zhang et al. show experimentally that their algorithm is much faster than existing algorithms, for similar quality of the built graph.

When it comes to building a k-nn graph from a big unstructured text dataset, none of the previous algorithms offers a efficient solution. Algorithms that rely on indexes are hard to implement in parallel on a distributed memory architecture like MapReduce. As LSH functions are defined only for some similarity measures ($l_p$, Mahalanobis distance, kernel similarity, and $\chi^2$ distance), the algorithms relying on LSH cannot be used when it comes to build a k-nn graph from text data using edit distance (Levenshtein distance) or any similar distance metric (weighted Levenshtein distance, Jaro-Winkler distance, Hamming distance) as a similarity measure.

In the case of NNDescent, the MapReduce version of the algorithm requires two MR jobs per iteration, and multiple iterations to converge. Moreover, the algorithm requires to read and write a lot of data on disk between jobs. Although the sequential version of the algorithm proved to be very efficient, these constraints make it very inefficient when implemented in parallel. This will be confirmed during the experimental tests presented below.

\section{NNCTPH}

\subsection{Context Triggered Piecewise Hashing}
Context Triggered Piecewise Hashing (CTPH), also called Fuzzy Hashing, was originally developed by Tridgell as a spam email detector called SpamSum \cite{SpamSum}. The algorithm is used to build a database of hashes of known spams. When a new email is received, its hash is computed, and compared with the spam database. If a similar hash is found, the incoming email is considered as spam, and discarded.

The algorithm works by splitting a character string in chunks of variable length. The end point of a chunk is determined by a rolling hash. This rolling hash is based on the Adler-32 checksum used in zlib \cite{zlib} and uses a windows of 7 characters that slides over the input string. By using a rolling hash, the algorithm can perform auto resynchronisation if characters are inserted or deleted between two strings. If the value of the rolling hash matches a given value, the end of current chunk is found.

Each chunk is hashed into a single character (out of 64). The block hash used is based on the Fowler/Noll/Vo (FNV) hash function \cite{FNV}.

The final hash of the input string is the sequence of these segment hashes. In the original algorithm, the matching value for the rolling hashing is chosen in such a way that the produced hash has a length of 64 characters.

Kornblum later implemented the algorithm under the name ssdeep \cite{ssdeep}. In \cite{Kornblum200691}, he uses the algorithm to identify almost identical files to support computer forensics.

\section{Experimental evaluation}

\subsection{Performance measure}

Like in \cite{Dong2011}, we use recall to measure the accuracy of algorithms. The ground truth is the true k-NN graph obtained using the naive, brute-force, algorithm. The recall of a single node is the number of its correct edges divided by $k$. The recall of an approximate k-NN graph is the average recall of all nodes.

To compare computational cost, as the number of similarities to compute depends on the size of the dataset, we use the scan rate, defined as follows:

$$ \text{scan rate} = \frac{\text{\# similarity evaluations}}{n (n-1) / 2} $$

where $n (n-1) / 2$ is the number of similarities computed by the naive algorithm.

\subsection{Results}
\begin{table}[h]
  \caption{Running time and scan rate of NNCTPH algorithm}
  \label{table:nnctph} 
  \centering
  \begin{tabular}{p{3.5cm} *{4}{p{2.5cm}}}
    \hline
                    	& \textbf{spam200k}   	& \textbf{spam400k}   	& \textbf{spam600k} 	& \textbf{spam800k}  	\\
    \hline
    \# spams           	& 200.000             	& 400.000             	& 600.000		& 800.000            	\\
    Running time (sec)  & 97	           	& 316           	& 675			& 1093          	\\
    \# similarities	& 88.262.345		& 359.577.959		& 773.200.303		& 1.346.569.275		\\
    Scan rate		& 0.44\%		& 0.45\%		& 0.43\%		& 0.42\%		\\
    \hline
  \end{tabular} 
\end{table}

\section{Conclusions}

\acks{This work has been partially supported by the EU project BigFoot (FP7-ICT-317858).}

\bibliography{bigmine2014_debatty}


\end{document}
