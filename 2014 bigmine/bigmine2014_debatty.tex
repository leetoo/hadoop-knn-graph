%\documentclass[wcp,gray]{jmlr} % test grayscale version
\documentclass[wcp]{jmlr}

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

%\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

% --------------------- Tibo
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{array}

\usepackage{hyperref}

\usepackage{pgfplots}
\usepackage{pgfplotstable}
%\usetikzlibrary{plotmarks}

\usepackage{relsize}
\usepackage{amssymb}
\usepackage{flushend}

\usepackage{xcolor}
\usepackage{changebar}

\newcommand{\removed}[1]{\cbstart\removedfragile{#1}\cbend{}}
\newcommand{\removedfragile}[1]{{\color{red}{#1}}{}}
\newcommand{\added}[1]{\cbstart\addedfragile{#1}\cbend{}}
\newcommand{\addedfragile}[1]{{\color{green!50!black}{#1}}{}}
\newcommand{\changed}[2]{\added{#1}\removed{#2}}
% --------------------- Tibo


% The booktabs package is used by this sample document
% (it provides \toprule, \midrule and \bottomrule).
% Remove the next line if you don't require it.
%\usepackage{booktabs}

% The siunitx package is used by this sample document
% to align numbers in a column by their decimal point.
% Remove the next line if you don't require it.
%\usepackage[load-configurations=version-1]{siunitx} % newer version
%\usepackage{siunitx}

% The following command is just for this sample document:
%\newcommand{\cs}[1]{\texttt{\char`\\#1}}

\jmlrvolume{29}
\jmlryear{2014}
\jmlrworkshop{BIGMINE 2014}

\title[Short Title]{Efficient Graph Building from Text Data}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \author{\Name{Author Name1} \Email{abc@sample.com}\and
 %  \Name{Author Name2} \Email{xyz@sample.com}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \author{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \Name{Author Name4} \Email{an4@sample.com}\\
 %  \Name{Author Name5} \Email{an5@sample.com}\\
 %  \Name{Author Name6} \Email{an6@sample.com}\\
 %  \Name{Author Name7} \Email{an7@sample.com}\\
 %  \Name{Author Name8} \Email{an8@sample.com}\\
 %  \Name{Author Name9} \Email{an9@sample.com}\\
 %  \Name{Author Name10} \Email{an10@sample.com}\\
 %  \Name{Author Name11} \Email{an11@sample.com}\\
 %  \Name{Author Name12} \Email{an12@sample.com}\\
 %  \Name{Author Name13} \Email{an13@sample.com}\\
 %  \Name{Author Name14} \Email{an14@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
  \author{\Name{Thibault Debatty} \Email{thibault.debatty@rma.ac.be}\\
  \addr Royal Military Academy, Brussels, Belgium
  \AND
  \Name{Pietro Michiardi} \Email{pietro.michiardi@eurecom.fr}\\
  \addr EURECOM, Campus SophiaTech, France
  \AND
  \Name{Olivier Thonnard} \Email{olivier\_thonnard@symantec.com}\\
  \addr Symantec Research Labs, Sophia Antipolis, France
  \AND
  \Name{Wim Mees} \Email{wim.mees@rma.ac.be}\\
  \addr Royal Military Academy, Brussels, Belgium
 }

%\editors{List of editors' names}

\begin{document}

\maketitle

\begin{abstract}
This is the abstract for this article.
\end{abstract}

\begin{keywords}
List of keywords
\end{keywords}

\section{Introduction}
A graph is a mathematical structure used to represent relations between objects. A graph is made up of nodes (or vertices) connected with edges. In some cases, the edges have a weight, resulting in a weighted graph. Graph theory is a very ancient topic, dating back from 1736. %http://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg
It has received a strong highlight these last years with the explosion of social networks like Facebook and Twitter. Indeed, the data generated by these networks can easily be formatted as a graph, making graph algorithms a premium choice for processing the data.

As a consequence, a lot of algorithms have been developed, and implemented, to efficiently analyze this kind of data, either sequentially or in parallel. XXXXXXX add examples of graph algorithms connected components, clustering, ...

In a lot of cases, the data to analyze is not formatted as a graph. Nevertheless, it can be interesting to convert the dataset into a graph, to be able to use these algorithms. In most cases (XXXXXXXXX add reference), the trick used is to build a k-nearest neighbors graph (k-NN graph), a graph where each node is connected to (has an edge to) its k nearest neighbors, according to a given similarity metric.

\section{Related work}

\subsection{Nearest neighbor search}
k-NN graph building algorithms are of course closely related to nearest neighbor search algorithms, and also to the k-nearest neighbors method.

The nearest-neighbor search problem is formally defined as follows: given a set $S$ of points in a space $M$ and a so-called query point $q \in M$, find the closest point in $S$ to $q$. The k-NN search is a direct generalization of this problem, where we need to find the k closest points.

But k-NN search is also a method used for classification and regression. In k-NN classification, the algorithm assigns to an object the class which is most common amongst its k nearest neighbors in the training set. In the case of regression, the value computed by the algorithm is the average value of the k nearest neighbors of the object in the training set.

Different methods exist to find the nearest neighbor, or the k-nearest neighbors, of a point. The naive method, also called linear search, consists in computing the distance between the query point and every other point in the set, keeping track of the "best so far" (or k "best so far").

Some techniques rely on the branch and bond algorithm, with some kind of index to partition the space $M$. For example, a k-d tree, that recursively partition the space into equally sized sub-spaces can be used to speedup search.

XXXXXXX ref:  Andrew Moore. "An introductory tutorial on KD trees" and Lee, D. T.; Wong, C. K. (1977). "Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees". Acta Informatica 9 (1): 23â€“29. doi:10.1007/BF00263763.

R-trees can also be used for euclidean spaces. In the case of metric spaces, vantage-point trees and BK-trees can be used.

XXXXXX add explanation and references!

Finally, some techniques use locality-sensitive hashing (LSH).
XXXXX ref:  A. Rajaraman and J. Ullman (2010). "Mining of Massive Datasets, Ch. 3.".
LSH is originally a method used to perform probabilistic dimension reduction of high-dimensional data. The basic idea is to hash the input items so that similar items are mapped to the same buckets with high probability. At the opposite of conventional hash functions, such as those used in cryptography, in the case of LSH the goal is to maximize the probability of "collision" between similar items.

\subsection{k-NN graph building algorithms}

The most naive way to build a k-NN graph is of course using brute force to compute all pairwise similarities. This method has a computational cost of $O(n^2)$.

A more subtle way consists in iteratively using a nearest neighbor search algorithm, like the ones presented above, to find the neighbors of all nodes in the dataset.

Lately, different algorithms have been proposed specifically to efficiently build an approximate k-NN graph from a dataset. Most of them will naturally share similarities with nearest neighbor search algorithms.

XXXX add references to algorithms that uses indexes.

A versatile algorithm to efficiently compute an approximate k-NN graph is described in \cite{Dong2011}. The algorithm, called NN-Descent, starts by creating edges between random nodes. Then, for each node, it computes the similarity between all neighbors of the current neighbors, to find better edges. The algorithm iterates until it cannot find better edges anymore. The main advantage of this algorithm is that it works with any similarity measure. Dong et al. experimentally found the computational cost is around $O(n^{1.14})$.

They also propose a MapReduce version of the algorithm. Internally, instead of working with edges, each node has a neighbors list, a list of candidate neighbors. The algorithm first creates a random neighbors list for each node. Then each iteration is realized with two MapReduce jobs. First, the mapper emits the initial neighbors list, and reverses the neighbors list to produce new candidate neighbors. The reducer merges all neighbors for each node, to produce a new extended neighbors list. In the second job, the mappers compute and emit the pairwise similarity between all elements of each neighbors list. Finally, the reducer merges the neighbors of each key node, keeping only the $k$ neighbors with the highest similarity.

Various authors propose algorithms relying on locality-sensitive hashing.

In \cite{Hsieh2012}, the authors propose a MapReduce algorithm that first bins the scale-invariant feature transform (SIFT) description of images into overlapping pools. The algorithm then computes the pairwise similarity between images in the same bucket to build the k-nn graph of images. For binning, the algorithm uses MinHash, a variant of LSH that uses Jaccard coefficient as similarity measure.

Similarly, in \cite{Zhang2013}, the authors use LSH to divide the dataset into small groups. Then, inside these small groups, they use the algorithm proposed by Dong et al. to build the k-nn graph. As groups are not overlapping, the constructed graph is a union of multiple isolated small graphs. To bind the final graph, and improve the approximation quality, the division is repeated several times to generate multiple approximate graphs, which are finally combined to produce the final graph.

Furthermore, the authors propose a method to produce equally sized groups, thus alleviating the computational cost of skewed data. They first project the item's hash code on a random direction. Then they sort items by their projection values. Finally, they divide this sequence of items into equally sized buckets. By doing so, the items with same hash code will fall in the same bucket with a high probability.

Finally, Zhang et al. show experimentally that their algorithm is much faster than existing algorithms, for similar quality of the built graph. But the main drawback of the algorithm is that LSH functions are defined only for some similarity measures: $l_p$, Mahalanobis distance, kernel similarity, and $\chi^2$ distance.


\section{NNCTPH}

\subsection{Context Triggered Piecewise Hashing}

\section{Experimental evaluation}

\subsection{Performance measure}

Like in \cite{Dong2011}, we use recall to measure the accuracy of algorithms. The ground truth is the true k-NN graph obtained using the naive, brute-force, algorithm. The recall of a single node is the number of its correct edges divided by $k$. The recall of an approximate k-NN graph is the average recall of all nodes.

To compare computational cost, as the number of similarities to compute depends on the size of the dataset, we use the scan rate, defined as follows:

$$ \text{scan rate} = \frac{\text{\# similarity evaluations}}{n (n-1) / 2} $$

where $n (n-1) / 2$ is the number of similarities computed by the naive algorithm.

\subsection{Results}
\begin{table}[h]
  \caption{Running time and scan rate of NNCTPH algorithm}
  \label{table:nnctph} 
  \centering
  \begin{tabular}{p{3.5cm} *{4}{p{2.5cm}}}
    \hline
                    	& \textbf{spam200k}   	& \textbf{spam400k}   	& \textbf{spam600k} 	& \textbf{spam800k}  	\\
    \hline
    \# spams           	& 200.000             	& 400.000             	& 600.000		& 800.000            	\\
    Running time (sec)  & 97	           	& 316           	& 675			& 1093          	\\
    \# similarities	& 88.262.345		& 359.577.959		& 773.200.303		& 1.346.569.275		\\
    Scan rate		& 0.44\%		& 0.45\%		& 0.43\%		& 0.42\%		\\
    \hline
  \end{tabular} 
\end{table}

\section{Conclusions}

\acks{This work has been partially supported by the EU project BigFoot (FP7-ICT-317858).}

\bibliography{bigmine2014_debatty}


\end{document}
